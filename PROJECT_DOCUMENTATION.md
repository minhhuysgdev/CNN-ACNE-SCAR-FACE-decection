# ğŸ“‹ TÃ i Liá»‡u MÃ´ Táº£ Dá»± Ãn: Acne Classification Using CNN

## ğŸ“Œ Tá»•ng Quan

**Má»¥c tiÃªu:** XÃ¢y dá»±ng mÃ´ hÃ¬nh deep learning sá»­ dá»¥ng CNN Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c loáº¡i má»¥n, Ä‘á»“ng thá»i giáº£i quyáº¿t váº¥n Ä‘á» máº¥t cÃ¢n báº±ng dá»¯ liá»‡u thÃ´ng qua oversampling vÃ  data augmentation.

**Dataset:** Acne Dataset Image tá»« Kaggle (tiswan14/acne-dataset-image)

**Sá»‘ lá»›p phÃ¢n loáº¡i:** 5 loáº¡i má»¥n
- Blackheads (Má»¥n Ä‘áº§u Ä‘en)
- Cyst (Má»¥n nang)
- Papules (Má»¥n sáº§n)
- Pustules (Má»¥n má»§)
- Whiteheads (Má»¥n Ä‘áº§u tráº¯ng)

---

## ğŸ“‚ Cáº¥u TrÃºc Notebook

### **Cell 0:** Metadata
- Link tham kháº£o Ä‘áº¿n Kaggle notebook gá»‘c

### **Cell 1:** Giá»›i thiá»‡u
- MÃ´ táº£ má»¥c tiÃªu cá»§a dá»± Ã¡n

### **Cell 2-4:** Import Libraries vÃ  Setup
- **Cell 3:** CÃ i Ä‘áº·t `kagglehub` Ä‘á»ƒ táº£i dataset
- **Cell 4:** Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
  - `tensorflow` vÃ  `keras` cho deep learning
  - `numpy`, `matplotlib` cho xá»­ lÃ½ dá»¯ liá»‡u vÃ  visualization
  - `sklearn` cho class weights computation

### **Cell 5-8:** Táº£i vÃ  Thiáº¿t Láº­p Dataset
- **Cell 6:** Táº£i dataset tá»« Kaggle sá»­ dá»¥ng `kagglehub`
- **Cell 8:** Äá»‹nh nghÄ©a cÃ¡c Ä‘Æ°á»ng dáº«n:
  - `base_dir`: ThÆ° má»¥c gá»‘c chá»©a dataset
  - `train_dir`: ThÆ° má»¥c training set
  - `valid_dir`: ThÆ° má»¥c validation set
  - `test_dir`: ThÆ° má»¥c test set

### **Cell 10-11:** Load Datasets
- **ThÃ´ng sá»‘:**
  - `BATCH_SIZE = 32`
  - `IMAGE_SIZE = 128` (128x128 pixels)
- Load datasets vá»›i `image_dataset_from_directory`
- XÃ¡c Ä‘á»‹nh `class_names` tá»« dataset

### **Cell 12-13:** Visualization
- Hiá»ƒn thá»‹ 9 áº£nh máº«u tá»« training set

### **Cell 14-16:** PhÃ¢n TÃ­ch PhÃ¢n Phá»‘i Dá»¯ Liá»‡u
- Äáº¿m sá»‘ lÆ°á»£ng áº£nh trong má»—i lá»›p
- Nháº­n xÃ©t vá» sá»± máº¥t cÃ¢n báº±ng dá»¯ liá»‡u

### **Cell 17-18:** TÃ­nh Class Weights
- Sá»­ dá»¥ng `compute_class_weight` vá»›i strategy 'balanced'
- TÃ­nh toÃ¡n trá»ng sá»‘ cho tá»«ng lá»›p Ä‘á»ƒ xá»­ lÃ½ imbalance

### **Cell 19-20:** Preprocessing
- Normalization: Rescaling pixel values vá» [0, 1] báº±ng cÃ¡ch chia cho 255

### **Cell 21-22:** Data Augmentation
- **CÃ¡c ká»¹ thuáº­t augmentation:**
  - `RandomFlip`: Láº­t ngang vÃ  dá»c
  - `RandomRotation`: Xoay vá»›i gÃ³c Â±15% (0.15)
  - `RandomZoom`: Zoom vá»›i tá»· lá»‡ Â±15% (0.15)

### **Cell 23-25:** Oversampling
- Xá»­ lÃ½ lá»›p thiá»ƒu sá»‘ (Whiteheads) báº±ng cÃ¡ch:
  - TÃ¡ch dataset theo tá»«ng lá»›p
  - Láº·p láº¡i lá»›p Whiteheads Ä‘á»ƒ Ä‘áº¡t sá»‘ lÆ°á»£ng báº±ng lá»›p Ä‘a sá»‘ nháº¥t
  - Káº¿t há»£p láº¡i vÃ  shuffle
  - Ãp dá»¥ng data augmentation
  - Batch vÃ  prefetch Ä‘á»ƒ tá»‘i Æ°u performance

### **Cell 26:** Tá»‘i Æ¯u Dataset
- Cache vÃ  prefetch cho validation vÃ  test sets

### **Cell 27-28:** XÃ¢y Dá»±ng MÃ´ HÃ¬nh CNN

#### **Kiáº¿n TrÃºc MÃ´ HÃ¬nh:**

```
Sequential Model:
â”œâ”€â”€ Conv2D(32 filters, 3x3) + ReLU
â”œâ”€â”€ MaxPooling2D(2x2)
â”œâ”€â”€ Conv2D(64 filters, 3x3) + ReLU
â”œâ”€â”€ MaxPooling2D(2x2)
â”œâ”€â”€ Conv2D(128 filters, 3x3) + ReLU
â”œâ”€â”€ MaxPooling2D(2x2)
â”œâ”€â”€ Conv2D(128 filters, 3x3) + ReLU
â”œâ”€â”€ MaxPooling2D(2x2)
â”œâ”€â”€ Conv2D(256 filters, 3x3) + ReLU
â”œâ”€â”€ MaxPooling2D(2x2)
â”œâ”€â”€ Flatten()
â”œâ”€â”€ Dense(128 units) + ReLU + L2 Regularization (0.001)
â”œâ”€â”€ Dropout(0.5)
â””â”€â”€ Dense(5 units) + Softmax
```

#### **ThÃ´ng Sá»‘ Compile:**
- **Optimizer:** Adam (default learning_rate = 0.001)
- **Loss Function:** Sparse Categorical Crossentropy
- **Metrics:** Accuracy

### **Cell 29-30:** Training

#### **Callbacks:**
1. **EarlyStopping:**
   - Monitor: `val_loss`
   - Patience: 5 epochs
   - `restore_best_weights=True`

2. **ReduceLROnPlateau:**
   - Monitor: `val_loss`
   - Factor: 0.5 (giáº£m learning rate má»™t ná»­a)
   - Patience: 3 epochs
   - Min learning rate: 1e-6

#### **Training Parameters:**
- **Epochs:** 50 (cÃ³ thá»ƒ dá»«ng sá»›m náº¿u EarlyStopping kÃ­ch hoáº¡t)
- **Training Data:** `balanced_ds` (Ä‘Ã£ oversample vÃ  augment)
- **Validation Data:** `valid_ds`
- **Class Weights:** Ãp dá»¥ng Ä‘á»ƒ xá»­ lÃ½ imbalance
- **Verbose:** 2 (hiá»ƒn thá»‹ má»™t dÃ²ng má»—i epoch)

### **Cell 31-32:** ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh
- Evaluate trÃªn test set
- TÃ­nh test accuracy vÃ  loss

### **Cell 33-36:** PhÃ¢n TÃ­ch Káº¿t Quáº£
- **Confusion Matrix:** Ma tráº­n nháº§m láº«n Ä‘á»ƒ phÃ¢n tÃ­ch lá»—i phÃ¢n loáº¡i
- **Classification Report:** BÃ¡o cÃ¡o chi tiáº¿t vá» precision, recall, F1-score cho tá»«ng lá»›p

### **Cell 37-38:** Visualization Training History
- Váº½ biá»ƒu Ä‘á»“ accuracy vÃ  loss qua cÃ¡c epochs
- So sÃ¡nh training vÃ  validation metrics

---

## ğŸ“Š ThÃ´ng Sá»‘ Dataset

### **PhÃ¢n Phá»‘i Dá»¯ Liá»‡u:**

| Class | Train | Validation | Test | Total |
|-------|-------|------------|------|-------|
| **Blackheads** | 735 | 240 | 265 | 1,240 |
| **Cyst** | 645 | 206 | 189 | 1,040 |
| **Papules** | 621 | 209 | 202 | 1,032 |
| **Pustules** | 584 | 217 | 205 | 1,006 |
| **Whiteheads** | 193 | 49 | 57 | 299 |
| **TOTAL** | **2,778** | **921** | **918** | **4,617** |

### **Váº¥n Äá» Máº¥t CÃ¢n Báº±ng:**
- Whiteheads chá»‰ cÃ³ 193 áº£nh trong training set (lá»›p thiá»ƒu sá»‘)
- Blackheads cÃ³ 735 áº£nh (lá»›p Ä‘a sá»‘ nháº¥t)
- Tá»· lá»‡: ~3.8:1 (Blackheads:Whiteheads)

### **Giáº£i PhÃ¡p:**
1. **Oversampling:** TÄƒng sá»‘ lÆ°á»£ng Whiteheads lÃªn ~735 (báº±ng vá»›i Blackheads)
2. **Class Weights:** Ãp dá»¥ng trá»ng sá»‘ cao hÆ¡n cho Whiteheads (2.88)
3. **Data Augmentation:** TÄƒng Ä‘a dáº¡ng dá»¯ liá»‡u cho táº¥t cáº£ cÃ¡c lá»›p

---

## ğŸ§  Kiáº¿n TrÃºc MÃ´ HÃ¬nh Chi Tiáº¿t

### **Input Shape:**
- `(128, 128, 3)` - áº¢nh RGB 128x128 pixels

### **Convolutional Layers:**

| Layer | Filters | Kernel Size | Activation | Output Shape |
|-------|---------|-------------|------------|--------------|
| Conv2D_1 | 32 | (3, 3) | ReLU | (126, 126, 32) |
| MaxPooling2D_1 | - | (2, 2) | - | (63, 63, 32) |
| Conv2D_2 | 64 | (3, 3) | ReLU | (61, 61, 64) |
| MaxPooling2D_2 | - | (2, 2) | - | (30, 30, 64) |
| Conv2D_3 | 128 | (3, 3) | ReLU | (28, 28, 128) |
| MaxPooling2D_3 | - | (2, 2) | - | (14, 14, 128) |
| Conv2D_4 | 128 | (3, 3) | ReLU | (12, 12, 128) |
| MaxPooling2D_4 | - | (2, 2) | - | (6, 6, 128) |
| Conv2D_5 | 256 | (3, 3) | ReLU | (4, 4, 256) |
| MaxPooling2D_5 | - | (2, 2) | - | (2, 2, 256) |

### **Fully Connected Layers:**

| Layer | Units | Activation | Regularization | Dropout |
|-------|-------|------------|----------------|---------|
| Flatten | - | - | - | - |
| Dense_1 | 128 | ReLU | L2(0.001) | - |
| Dropout | - | - | - | 0.5 |
| Dense_2 (Output) | 5 | Softmax | - | - |

### **Tá»•ng Sá»‘ Tham Sá»‘:**
- Cáº§n cháº¡y `model.summary()` Ä‘á»ƒ xem chi tiáº¿t sá»‘ lÆ°á»£ng parameters

---

## âš™ï¸ Hyperparameters

### **Data Parameters:**
- `BATCH_SIZE = 32`
- `IMAGE_SIZE = 128`
- `shuffle_buffer_size = 5000` (cho balanced dataset)

### **Model Parameters:**
- **L2 Regularization:** 0.001
- **Dropout Rate:** 0.5
- **Activation Functions:**
  - Convolutional layers: ReLU
  - Output layer: Softmax

### **Training Parameters:**
- **Optimizer:** Adam
- **Initial Learning Rate:** 0.001
- **Loss Function:** Sparse Categorical Crossentropy
- **Max Epochs:** 50
- **Class Weights:**
  - Blackheads: 0.756
  - Cyst: 0.861
  - Papules: 0.895
  - Pustules: 0.951
  - Whiteheads: 2.879

### **Callbacks Parameters:**
- **EarlyStopping:**
  - Monitor: `val_loss`
  - Patience: 5
  - Restore best weights: True
  
- **ReduceLROnPlateau:**
  - Monitor: `val_loss`
  - Factor: 0.5
  - Patience: 3
  - Min LR: 1e-6

### **Data Augmentation Parameters:**
- **RandomFlip:** `horizontal_and_vertical`
- **RandomRotation:** 0.15 (Â±15%)
- **RandomZoom:** 0.15 (Â±15%)

---

## ğŸ“ˆ Káº¿t Quáº£ MÃ´ HÃ¬nh

### **Test Performance:**
- **Test Accuracy:** 63.29%
- **Test Loss:** 0.868

### **Classification Report:**

| Class | Precision | Recall | F1-Score | Support |
|-------|-----------|--------|----------|---------|
| **Blackheads** | 0.78 | 0.65 | 0.71 | 265 |
| **Cyst** | 0.61 | 0.71 | 0.66 | 189 |
| **Papules** | 0.53 | 0.60 | 0.56 | 202 |
| **Pustules** | 0.53 | 0.47 | 0.50 | 205 |
| **Whiteheads** | 0.89 | 0.98 | 0.93 | 57 |
| **Macro Avg** | 0.67 | 0.68 | 0.67 | 918 |
| **Weighted Avg** | 0.64 | 0.63 | 0.63 | 918 |

### **Nháº­n XÃ©t:**
- âœ… **Whiteheads** cÃ³ performance tá»‘t nháº¥t (F1 = 0.93) - nhá» oversampling vÃ  class weights
- âœ… **Blackheads** cÃ³ precision cao (0.78) nhÆ°ng recall tháº¥p hÆ¡n (0.65)
- âš ï¸ **Papules** vÃ  **Pustules** cÃ³ performance tháº¥p hÆ¡n, cÃ³ thá»ƒ do Ä‘áº·c Ä‘iá»ƒm tÆ°Æ¡ng Ä‘á»“ng
- ğŸ“Š Overall accuracy: 63% - CÃ³ thá»ƒ cáº£i thiá»‡n thÃªm

---

## ğŸ”§ CÃ¡c Ká»¹ Thuáº­t ÄÆ°á»£c Sá»­ Dá»¥ng

### **1. Xá»­ LÃ½ Imbalanced Data:**
- âœ… Oversampling lá»›p thiá»ƒu sá»‘ (Whiteheads)
- âœ… Class weights trong training
- âœ… Data augmentation

### **2. Regularization:**
- âœ… L2 Regularization (0.001) trong Dense layer
- âœ… Dropout (0.5) Ä‘á»ƒ giáº£m overfitting

### **3. Optimization:**
- âœ… Adam optimizer vá»›i adaptive learning rate
- âœ… Learning rate scheduling (ReduceLROnPlateau)
- âœ… Early stopping Ä‘á»ƒ trÃ¡nh overfitting

### **4. Data Pipeline Optimization:**
- âœ… Dataset caching
- âœ… Prefetch Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ training
- âœ… Batch processing

---

## ğŸ“ Ghi ChÃº Quan Trá»ng

1. **Dataset Path:** Dataset Ä‘Æ°á»£c táº£i vá» tá»« Kaggle vÃ  lÆ°u táº¡i cache directory cá»§a kagglehub
2. **Normalization:** Pixel values Ä‘Æ°á»£c normalize vá» [0, 1] trÆ°á»›c khi training
3. **Data Augmentation:** Chá»‰ Ã¡p dá»¥ng cho training set, khÃ´ng Ã¡p dá»¥ng cho validation/test
4. **Class Weights:** ÄÆ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn phÃ¢n phá»‘i ban Ä‘áº§u cá»§a training set
5. **Oversampling:** Chá»‰ Ã¡p dá»¥ng cho training set, validation vÃ  test giá»¯ nguyÃªn phÃ¢n phá»‘i gá»‘c

---

## ğŸš€ HÆ°á»›ng PhÃ¡t Triá»ƒn

### **CÃ³ thá»ƒ cáº£i thiá»‡n:**
1. **TÄƒng kÃ­ch thÆ°á»›c áº£nh:** Thá»­ `IMAGE_SIZE = 224` hoáº·c `256`
2. **Transfer Learning:** Sá»­ dá»¥ng pre-trained models (VGG16, ResNet50, EfficientNet)
3. **Tinh chá»‰nh kiáº¿n trÃºc:** ThÃªm BatchNormalization, GlobalAveragePooling2D
4. **Ensemble Methods:** Káº¿t há»£p nhiá»u mÃ´ hÃ¬nh
5. **TÄƒng sá»‘ lÆ°á»£ng dá»¯ liá»‡u:** Thu tháº­p thÃªm dá»¯ liá»‡u, Ä‘áº·c biá»‡t cho cÃ¡c lá»›p cÃ³ performance tháº¥p
6. **Hyperparameter Tuning:** Tá»‘i Æ°u learning rate, batch size, dropout rate
7. **Advanced Augmentation:** ThÃªm cÃ¡c ká»¹ thuáº­t nhÆ° color jittering, elastic transformation

---

## ğŸ“š Tham Kháº£o

- **Dataset:** [Kaggle - Acne Dataset Image](https://www.kaggle.com/datasets/tiswan14/acne-dataset-image)
- **Original Notebook:** [Kaggle Notebook](https://www.kaggle.com/code/zulqarnain11/acne-classification-using-cnn)
- **Framework:** TensorFlow/Keras
- **Python Version:** 3.13

---

*TÃ i liá»‡u Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng tá»« notebook `acne-classification-using-cnn.ipynb`*

